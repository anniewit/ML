{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QLearning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anniewit/ML/blob/master/QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbrFQ93eXOCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fizhUbL7XA5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rand\n",
        "\n",
        "def generate_field(x, y, num_rewards, max_reward):\n",
        "    \"\"\"\n",
        "    Generate a random game field with rewards.\n",
        "    \n",
        "    Args:\n",
        "        x (int):            x dimension of the field\n",
        "        y (int):            y dimension of the field \n",
        "        num_rewards (int):  the number of rewards that should be randomly placed\n",
        "        max_reward (int):   the maximum reward that can be placed \n",
        "        \n",
        "    Returns:\n",
        "        ndarray: A field with randomly initialized rewards, the rest of the \n",
        "        entries is zero\n",
        "    \"\"\"\n",
        "    \n",
        "    # Change or comment out to get different random data in each run\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    field = np.zeros((y,x), dtype=np.uint8)\n",
        "    \n",
        "    for i in range(num_rewards):\n",
        "        field[rand.randint(y), rand.randint(x)] = rand.choice(max_reward)\n",
        "    \n",
        "    return field"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "97a707be2a7ba223f1b0c3d6618a16b4",
          "grade": true,
          "grade_id": "ex2a_solution",
          "locked": false,
          "points": 7,
          "schema_version": 1,
          "solution": true
        },
        "id": "tmxKhwr-XA5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    This class contains all the necessary methods to navigate through\n",
        "    a maze or game with the help of a little bit of Q-Learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field, actions, gamma):\n",
        "        \"\"\"\n",
        "        Initializes the QLearning Algorithm with the necessary parameters.\n",
        "        All q values are stored in self.q - this is an array that has\n",
        "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
        "        in each field. The starting position self.pos is randomly initialized.\n",
        "        \n",
        "        Args:\n",
        "            field (ndarray):  the map (holds rewards?)\n",
        "            actions (list):   the available actions\n",
        "            gamma (float):    the gamma in the lecture slides\n",
        "        \n",
        "        Returns:\n",
        "            QLearning: An instance that can be used for Q-Learning on the field\n",
        "        \"\"\"\n",
        "        # q stores the q_values for each action in each space of the field.\n",
        "        self.field = field\n",
        "     ##   print(\"field\",field.shape)\n",
        "        self.actions = actions\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Remember the map extend for further navigation.\n",
        "        self.map_y = self.field.shape[0]\n",
        "        self.map_x = self.field.shape[1]\n",
        "        \n",
        "        # Create q value matrix.\n",
        "        self.q = np.zeros((len(self.actions), self.map_y, self.map_x))\n",
        "  ###      print(\"q-shape: \", self.q.shape)\n",
        "\n",
        "        # Start on a random position in the field.\n",
        "        self.pos = [np.random.randint(self.map_y), np.random.randint(self.map_x)]\n",
        "        self.fig, self.axes = plt.subplots(3, 3, num='QLearning State')\n",
        "        for ax in self.axes.flat:\n",
        "            ax.axis('off')\n",
        "\n",
        "    def get_coordinates(self, position, action):\n",
        "        \"\"\"\n",
        "        Returns the coordinates that follow a certain action, depending\n",
        "        on the current position of the learner. If the border is reached\n",
        "        the agent just stops there.\n",
        "        \n",
        "        Args:\n",
        "            position (pair):  the current position\n",
        "            action (string):  the action that should be performed (one of: 'up', 'down', ...)\n",
        "            \n",
        "        Returns:\n",
        "            pair of int: the updated coordinates\n",
        "        \"\"\"\n",
        "        # return the right new coordinates depending on the position\n",
        "        # YOUR CODE HERE\n",
        "        ##print(\"old position \", position, \" \\t action \", action )\n",
        "        (y,x) = position\n",
        "        \n",
        "        if action == 'up' and y < self.map_y - 1: \n",
        "            return(y + 1, x)\n",
        "        if action == 'down' and y > 0:\n",
        "            return(y - 1, x)\n",
        "        if action == 'right' and x < self.map_x - 1:\n",
        "            return(y, x + 1)\n",
        "        if action == 'left' and x > 0:\n",
        "            return(y, x - 1)\n",
        "        # if cannot move (border reached), stay at position\n",
        "        else:\n",
        "            return position\n",
        "    \n",
        "    def action_index(self, action):\n",
        "        \"\"\"convert  action to a number in order to story entries in q-value matrix\"\"\"\n",
        "        return self.actions.index(action)\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Implementation of the update step. Closely follows the Algorithm described on\n",
        "        ML-10 Sl.18. Note that you have attributes available as specified in the\n",
        "        __init__ method of this class, in addition to that is the FIELD variable that\n",
        "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
        "        stores the available actions.\n",
        "        \"\"\"\n",
        "        # Select a random action that should be performed next.\n",
        "        # Be careful to handle the case where you hit the wall!\n",
        "        # YOUR CODE HERE\n",
        "        action = np.random.choice(self.actions)\n",
        "\n",
        "        # Receive the reward for the new position from the field.\n",
        "        # YOUR CODE HERE\n",
        "        old_y, old_x = self.pos\n",
        "        (new_y, new_x) = self.get_coordinates(self.pos, action)\n",
        "        \n",
        "        # if wall was hit\n",
        "        if new_y == old_y and new_x == old_x: \n",
        "            return\n",
        "        \n",
        "        reward = field[new_y, new_x]\n",
        "        \n",
        "        \n",
        "        # Update the q-value for the performed action.\n",
        "        # YOUR CODE HERE\n",
        "        self.q[self.action_index(action), old_y, old_x] = reward + self.gamma * np.amax([self.q[a, new_y, new_x] for a in range(len(self.actions))])\n",
        "\n",
        "        # Update the position of the player to the new field.\n",
        "        # YOUR CODE HERE\n",
        "        self.pos = (new_y, new_x)\n",
        "\n",
        "\n",
        "    def plot(self):\n",
        "        \"\"\"\n",
        "        Plots the current state.\n",
        "        \"\"\"\n",
        "        for i, action in enumerate(self.actions):\n",
        "            ax = self.axes.flat[2*i + 1]\n",
        "            ax.set(title=action)\n",
        "            ax.imshow(self.q[i,:,:], interpolation='None')\n",
        "\n",
        "        self.fig.canvas.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}